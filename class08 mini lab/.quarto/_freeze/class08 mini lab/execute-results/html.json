{
  "hash": "f1c6b7b5c6c5a31fcaf1a7c3b1ef3829",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"class08 mini lab\"\nauthor: \"Chris Yi (A16849780)\"\noutput: pdf\n---\n\n\nToday we will do a complete analysis of some breast cancer biopsy data but first let's revisit the main PCA function in R `prcomp()` and see what `scale=TRUE/FALSE` does. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(mtcars)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n```\n\n\n:::\n:::\n\n\n\nFind the mean value per column of the dataset.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\napply(mtcars, 2, mean)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       mpg        cyl       disp         hp       drat         wt       qsec \n 20.090625   6.187500 230.721875 146.687500   3.596563   3.217250  17.848750 \n        vs         am       gear       carb \n  0.437500   0.406250   3.687500   2.812500 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\napply(mtcars, 2, sd)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n        mpg         cyl        disp          hp        drat          wt \n  6.0269481   1.7859216 123.9386938  68.5628685   0.5346787   0.9784574 \n       qsec          vs          am        gear        carb \n  1.7869432   0.5040161   0.4989909   0.7378041   1.6152000 \n```\n\n\n:::\n:::\n\n\n\nIt is clear that \"disp\" and \"hp\" have the highest mean values and the highest standard deviation values. They will likely dominate any analysis I do on this dataset. Let's see \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npc.noscale <- prcomp(mtcars,scale = FALSE )\npc.scale <- prcomp(mtcars, scale=TRUE)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbiplot(pc.noscale)\n```\n\n::: {.cell-output-display}\n![](class08-mini-lab_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\npc.noscale$rotation[,1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         mpg          cyl         disp           hp         drat           wt \n-0.038118199  0.012035150  0.899568146  0.434784387 -0.002660077  0.006239405 \n        qsec           vs           am         gear         carb \n-0.006671270 -0.002729474 -0.001962644 -0.002604768  0.005766010 \n```\n\n\n:::\n:::\n\n\n\nplot the loadings \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\nr1 <- as.data.frame(pc.noscale$rotation)\nr1$names <- rownames(pc.noscale$rotation)\n\nggplot(r1) + aes(PC1, names) + geom_col()\n```\n\n::: {.cell-output-display}\n![](class08-mini-lab_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nr2 <- as.data.frame(pc.scale$rotation)\nr2$names <- rownames(pc.scale$rotation)\n\nggplot(r2) + aes(PC1, names) + geom_col()\n```\n\n::: {.cell-output-display}\n![](class08-mini-lab_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n\n```{.r .cell-code}\nr2\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            PC1         PC2         PC3          PC4         PC5         PC6\nmpg  -0.3625305  0.01612440 -0.22574419 -0.022540255  0.10284468 -0.10879743\ncyl   0.3739160  0.04374371 -0.17531118 -0.002591838  0.05848381  0.16855369\ndisp  0.3681852 -0.04932413 -0.06148414  0.256607885  0.39399530 -0.33616451\nhp    0.3300569  0.24878402  0.14001476 -0.067676157  0.54004744  0.07143563\ndrat -0.2941514  0.27469408  0.16118879  0.854828743  0.07732727  0.24449705\nwt    0.3461033 -0.14303825  0.34181851  0.245899314 -0.07502912 -0.46493964\nqsec -0.2004563 -0.46337482  0.40316904  0.068076532 -0.16466591 -0.33048032\nvs   -0.3065113 -0.23164699  0.42881517 -0.214848616  0.59953955  0.19401702\nam   -0.2349429  0.42941765 -0.20576657 -0.030462908  0.08978128 -0.57081745\ngear -0.2069162  0.46234863  0.28977993 -0.264690521  0.04832960 -0.24356284\ncarb  0.2140177  0.41357106  0.52854459 -0.126789179 -0.36131875  0.18352168\n              PC7          PC8          PC9        PC10         PC11 names\nmpg   0.367723810 -0.754091423  0.235701617  0.13928524 -0.124895628   mpg\ncyl   0.057277736 -0.230824925  0.054035270 -0.84641949 -0.140695441   cyl\ndisp  0.214303077  0.001142134  0.198427848  0.04937979  0.660606481  disp\nhp   -0.001495989 -0.222358441 -0.575830072  0.24782351 -0.256492062    hp\ndrat  0.021119857  0.032193501 -0.046901228 -0.10149369 -0.039530246  drat\nwt   -0.020668302 -0.008571929  0.359498251  0.09439426 -0.567448697    wt\nqsec  0.050010522 -0.231840021 -0.528377185 -0.27067295  0.181361780  qsec\nvs   -0.265780836  0.025935128  0.358582624 -0.15903909  0.008414634    vs\nam   -0.587305101 -0.059746952 -0.047403982 -0.17778541  0.029823537    am\ngear  0.605097617  0.336150240 -0.001735039 -0.21382515 -0.053507085  gear\ncarb -0.174603192 -0.395629107  0.170640677  0.07225950  0.319594676  carb\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbiplot(pc.scale)\n```\n\n::: {.cell-output-display}\n![](class08-mini-lab_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n\n> **Take-home**: Generally we always want to set `scale=TRUE` when we do this type of analysis to avoid our analyses being dominated by individual variables with the largest variance just due to their unit of measurement. \n\n# FNA breast cancer data \n\nLoad the data into R. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwisc.df <- read.csv(\"WisconsinCancer.csv\", row.names=1)\nhead(wisc.df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n         diagnosis radius_mean texture_mean perimeter_mean area_mean\n842302           M       17.99        10.38         122.80    1001.0\n842517           M       20.57        17.77         132.90    1326.0\n84300903         M       19.69        21.25         130.00    1203.0\n84348301         M       11.42        20.38          77.58     386.1\n84358402         M       20.29        14.34         135.10    1297.0\n843786           M       12.45        15.70          82.57     477.1\n         smoothness_mean compactness_mean concavity_mean concave.points_mean\n842302           0.11840          0.27760         0.3001             0.14710\n842517           0.08474          0.07864         0.0869             0.07017\n84300903         0.10960          0.15990         0.1974             0.12790\n84348301         0.14250          0.28390         0.2414             0.10520\n84358402         0.10030          0.13280         0.1980             0.10430\n843786           0.12780          0.17000         0.1578             0.08089\n         symmetry_mean fractal_dimension_mean radius_se texture_se perimeter_se\n842302          0.2419                0.07871    1.0950     0.9053        8.589\n842517          0.1812                0.05667    0.5435     0.7339        3.398\n84300903        0.2069                0.05999    0.7456     0.7869        4.585\n84348301        0.2597                0.09744    0.4956     1.1560        3.445\n84358402        0.1809                0.05883    0.7572     0.7813        5.438\n843786          0.2087                0.07613    0.3345     0.8902        2.217\n         area_se smoothness_se compactness_se concavity_se concave.points_se\n842302    153.40      0.006399        0.04904      0.05373           0.01587\n842517     74.08      0.005225        0.01308      0.01860           0.01340\n84300903   94.03      0.006150        0.04006      0.03832           0.02058\n84348301   27.23      0.009110        0.07458      0.05661           0.01867\n84358402   94.44      0.011490        0.02461      0.05688           0.01885\n843786     27.19      0.007510        0.03345      0.03672           0.01137\n         symmetry_se fractal_dimension_se radius_worst texture_worst\n842302       0.03003             0.006193        25.38         17.33\n842517       0.01389             0.003532        24.99         23.41\n84300903     0.02250             0.004571        23.57         25.53\n84348301     0.05963             0.009208        14.91         26.50\n84358402     0.01756             0.005115        22.54         16.67\n843786       0.02165             0.005082        15.47         23.75\n         perimeter_worst area_worst smoothness_worst compactness_worst\n842302            184.60     2019.0           0.1622            0.6656\n842517            158.80     1956.0           0.1238            0.1866\n84300903          152.50     1709.0           0.1444            0.4245\n84348301           98.87      567.7           0.2098            0.8663\n84358402          152.20     1575.0           0.1374            0.2050\n843786            103.40      741.6           0.1791            0.5249\n         concavity_worst concave.points_worst symmetry_worst\n842302            0.7119               0.2654         0.4601\n842517            0.2416               0.1860         0.2750\n84300903          0.4504               0.2430         0.3613\n84348301          0.6869               0.2575         0.6638\n84358402          0.4000               0.1625         0.2364\n843786            0.5355               0.1741         0.3985\n         fractal_dimension_worst\n842302                   0.11890\n842517                   0.08902\n84300903                 0.08758\n84348301                 0.17300\n84358402                 0.07678\n843786                   0.12440\n```\n\n\n:::\n:::\n\n\n\n> Q1. How many observations are in this dataset?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnrow(wisc.df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 569\n```\n\n\n:::\n:::\n\n\n\n> Q2. How many of the observations have a malignant diagnosis?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(wisc.df$diagnosis)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  B   M \n357 212 \n```\n\n\n:::\n:::\n\n\n\n> Q3. How many variables/features in the data are suffixed with _mean?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nncol(wisc.df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 31\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ncolnames(wisc.df)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"diagnosis\"               \"radius_mean\"            \n [3] \"texture_mean\"            \"perimeter_mean\"         \n [5] \"area_mean\"               \"smoothness_mean\"        \n [7] \"compactness_mean\"        \"concavity_mean\"         \n [9] \"concave.points_mean\"     \"symmetry_mean\"          \n[11] \"fractal_dimension_mean\"  \"radius_se\"              \n[13] \"texture_se\"              \"perimeter_se\"           \n[15] \"area_se\"                 \"smoothness_se\"          \n[17] \"compactness_se\"          \"concavity_se\"           \n[19] \"concave.points_se\"       \"symmetry_se\"            \n[21] \"fractal_dimension_se\"    \"radius_worst\"           \n[23] \"texture_worst\"           \"perimeter_worst\"        \n[25] \"area_worst\"              \"smoothness_worst\"       \n[27] \"compactness_worst\"       \"concavity_worst\"        \n[29] \"concave.points_worst\"    \"symmetry_worst\"         \n[31] \"fractal_dimension_worst\"\n```\n\n\n:::\n:::\n\n\n\nA useful function for this is `grep()` \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlength( grep(\"_mean\", colnames(wisc.df)) )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 10\n```\n\n\n:::\n:::\n\n\n\nBefore we go any further we need to exclude the diagonoses column from any future analysis - this tells us whether a sample to cancer or non-cancer\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiagnosis <- as.factor(wisc.df$diagnosis)\nhead(diagnosis)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] M M M M M M\nLevels: B M\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwisc.data <- wisc.df[,-1]\n```\n:::\n\n\n\nLet's see if we can cluster the `wisc.data` to find some structure in the dataset. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhc <- hclust(dist(wisc.data))\nplot(hc)\n```\n\n::: {.cell-output-display}\n![](class08-mini-lab_files/figure-html/unnamed-chunk-18-1.png){width=672}\n:::\n:::\n\n\n\n# Principal Component Analysis (PCA)\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwisc.pr <- prcomp(wisc.data, scale = T) \nsummary(wisc.pr)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nImportance of components:\n                          PC1    PC2     PC3     PC4     PC5     PC6     PC7\nStandard deviation     3.6444 2.3857 1.67867 1.40735 1.28403 1.09880 0.82172\nProportion of Variance 0.4427 0.1897 0.09393 0.06602 0.05496 0.04025 0.02251\nCumulative Proportion  0.4427 0.6324 0.72636 0.79239 0.84734 0.88759 0.91010\n                           PC8    PC9    PC10   PC11    PC12    PC13    PC14\nStandard deviation     0.69037 0.6457 0.59219 0.5421 0.51104 0.49128 0.39624\nProportion of Variance 0.01589 0.0139 0.01169 0.0098 0.00871 0.00805 0.00523\nCumulative Proportion  0.92598 0.9399 0.95157 0.9614 0.97007 0.97812 0.98335\n                          PC15    PC16    PC17    PC18    PC19    PC20   PC21\nStandard deviation     0.30681 0.28260 0.24372 0.22939 0.22244 0.17652 0.1731\nProportion of Variance 0.00314 0.00266 0.00198 0.00175 0.00165 0.00104 0.0010\nCumulative Proportion  0.98649 0.98915 0.99113 0.99288 0.99453 0.99557 0.9966\n                          PC22    PC23   PC24    PC25    PC26    PC27    PC28\nStandard deviation     0.16565 0.15602 0.1344 0.12442 0.09043 0.08307 0.03987\nProportion of Variance 0.00091 0.00081 0.0006 0.00052 0.00027 0.00023 0.00005\nCumulative Proportion  0.99749 0.99830 0.9989 0.99942 0.99969 0.99992 0.99997\n                          PC29    PC30\nStandard deviation     0.02736 0.01153\nProportion of Variance 0.00002 0.00000\nCumulative Proportion  1.00000 1.00000\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nbiplot(wisc.pr)\n```\n\n::: {.cell-output-display}\n![](class08-mini-lab_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n\nThis biplot sucks! We need to build our own PCA score plot of PC1 vs PC2\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nattributes(wisc.pr)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n$names\n[1] \"sdev\"     \"rotation\" \"center\"   \"scale\"    \"x\"       \n\n$class\n[1] \"prcomp\"\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(wisc.pr$x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n               PC1        PC2        PC3       PC4        PC5         PC6\n842302   -9.184755  -1.946870 -1.1221788 3.6305364  1.1940595  1.41018364\n842517   -2.385703   3.764859 -0.5288274 1.1172808 -0.6212284  0.02863116\n84300903 -5.728855   1.074229 -0.5512625 0.9112808  0.1769302  0.54097615\n84348301 -7.116691 -10.266556 -3.2299475 0.1524129  2.9582754  3.05073750\n84358402 -3.931842   1.946359  1.3885450 2.9380542 -0.5462667 -1.22541641\n843786   -2.378155  -3.946456 -2.9322967 0.9402096  1.0551135 -0.45064213\n                 PC7         PC8         PC9       PC10       PC11       PC12\n842302    2.15747152  0.39805698 -0.15698023 -0.8766305 -0.2627243 -0.8582593\n842517    0.01334635 -0.24077660 -0.71127897  1.1060218 -0.8124048  0.1577838\n84300903 -0.66757908 -0.09728813  0.02404449  0.4538760  0.6050715  0.1242777\n84348301  1.42865363 -1.05863376 -1.40420412 -1.1159933  1.1505012  1.0104267\n84358402 -0.93538950 -0.63581661 -0.26357355  0.3773724 -0.6507870 -0.1104183\n843786    0.49001396  0.16529843 -0.13335576 -0.5299649 -0.1096698  0.0813699\n                PC13         PC14         PC15        PC16        PC17\n842302    0.10329677 -0.690196797  0.601264078  0.74446075 -0.26523740\n842517   -0.94269981 -0.652900844 -0.008966977 -0.64823831 -0.01719707\n84300903 -0.41026561  0.016665095 -0.482994760  0.32482472  0.19075064\n84348301 -0.93245070 -0.486988399  0.168699395  0.05132509  0.48220960\n84358402  0.38760691 -0.538706543 -0.310046684 -0.15247165  0.13302526\n843786   -0.02625135  0.003133944 -0.178447576 -0.01270566  0.19671335\n                PC18       PC19        PC20         PC21        PC22\n842302   -0.54907956  0.1336499  0.34526111  0.096430045 -0.06878939\n842517    0.31801756 -0.2473470 -0.11403274 -0.077259494  0.09449530\n84300903 -0.08789759 -0.3922812 -0.20435242  0.310793246  0.06025601\n84348301 -0.03584323 -0.0267241 -0.46432511  0.433811661  0.20308706\n84358402 -0.01869779  0.4610302  0.06543782 -0.116442469  0.01763433\n843786   -0.29727706 -0.1297265 -0.07117453 -0.002400178  0.10108043\n                PC23         PC24         PC25         PC26        PC27\n842302    0.08444429  0.175102213  0.150887294 -0.201326305 -0.25236294\n842517   -0.21752666 -0.011280193  0.170360355 -0.041092627  0.18111081\n84300903 -0.07422581 -0.102671419 -0.171007656  0.004731249  0.04952586\n84348301 -0.12399554 -0.153294780 -0.077427574 -0.274982822  0.18330078\n84358402  0.13933105  0.005327110 -0.003059371  0.039219780  0.03213957\n843786    0.03344819 -0.002837749 -0.122282765 -0.030272333 -0.08438081\n                  PC28         PC29          PC30\n842302   -0.0338846387  0.045607590  0.0471277407\n842517    0.0325955021 -0.005682424  0.0018662342\n84300903  0.0469844833  0.003143131 -0.0007498749\n84348301  0.0424469831 -0.069233868  0.0199198881\n84358402 -0.0347556386  0.005033481 -0.0211951203\n843786    0.0007296587 -0.019703996 -0.0034564331\n```\n\n\n:::\n:::\n\n\n\nPlot PC1 vs PC2 the first two columns\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(wisc.pr$x[,1], wisc.pr$x[,2], col=diagnosis)\n```\n\n::: {.cell-output-display}\n![](class08-mini-lab_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\n\nMake a ggplot version of this score plot\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npc <- as.data.frame(wisc.pr$x)\n\nggplot(pc) + aes(PC1, PC2, col=diagnosis) + geom_point()\n```\n\n::: {.cell-output-display}\n![](class08-mini-lab_files/figure-html/unnamed-chunk-24-1.png){width=672}\n:::\n:::\n\n\n\n> Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(wisc.pr)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nImportance of components:\n                          PC1    PC2     PC3     PC4     PC5     PC6     PC7\nStandard deviation     3.6444 2.3857 1.67867 1.40735 1.28403 1.09880 0.82172\nProportion of Variance 0.4427 0.1897 0.09393 0.06602 0.05496 0.04025 0.02251\nCumulative Proportion  0.4427 0.6324 0.72636 0.79239 0.84734 0.88759 0.91010\n                           PC8    PC9    PC10   PC11    PC12    PC13    PC14\nStandard deviation     0.69037 0.6457 0.59219 0.5421 0.51104 0.49128 0.39624\nProportion of Variance 0.01589 0.0139 0.01169 0.0098 0.00871 0.00805 0.00523\nCumulative Proportion  0.92598 0.9399 0.95157 0.9614 0.97007 0.97812 0.98335\n                          PC15    PC16    PC17    PC18    PC19    PC20   PC21\nStandard deviation     0.30681 0.28260 0.24372 0.22939 0.22244 0.17652 0.1731\nProportion of Variance 0.00314 0.00266 0.00198 0.00175 0.00165 0.00104 0.0010\nCumulative Proportion  0.98649 0.98915 0.99113 0.99288 0.99453 0.99557 0.9966\n                          PC22    PC23   PC24    PC25    PC26    PC27    PC28\nStandard deviation     0.16565 0.15602 0.1344 0.12442 0.09043 0.08307 0.03987\nProportion of Variance 0.00091 0.00081 0.0006 0.00052 0.00027 0.00023 0.00005\nCumulative Proportion  0.99749 0.99830 0.9989 0.99942 0.99969 0.99992 0.99997\n                          PC29    PC30\nStandard deviation     0.02736 0.01153\nProportion of Variance 0.00002 0.00000\nCumulative Proportion  1.00000 1.00000\n```\n\n\n:::\n:::\n\n\n0.4427\n\n> Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?\n\n4 principal components\n\n> Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?\n\n7 principal components\n\n> Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbiplot(wisc.pr)\n```\n\n::: {.cell-output-display}\n![](class08-mini-lab_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n\nThe biplot is difficult to interpret because it overlays all variable vectors on top of the data points, making it cluttered. Using a scatter plot of PC1 vs. PC2 is a better visualization.\n\n> Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(wisc.pr$x[,1], wisc.pr$x[,3], col = diagnosis, \n     xlab = \"PC1\", ylab = \"PC3\")\n```\n\n::: {.cell-output-display}\n![](class08-mini-lab_files/figure-html/unnamed-chunk-27-1.png){width=672}\n:::\n:::\n\n\n\nPC1 vs. PC3 still shows some separation between malignant and benign cases, but it is less clear than PC1 vs. PC2, indicating that PC2 captures more variance related to diagnosis.\n\n> Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwisc.pr$rotation[\"concave.points_mean\", 1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -0.2608538\n```\n\n\n:::\n:::\n\n\n\n> Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?\n\n5 PC's\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.scaled <- scale(wisc.data)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.dist <- dist(data.scaled)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwisc.hclust <- hclust(data.dist, method = \"complete\")\n```\n:::\n\n\n\n> Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(wisc.hclust)\nabline(h = 15, col = \"red\", lty=2)\n```\n\n::: {.cell-output-display}\n![](class08-mini-lab_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n:::\n\n\nh = 15 \n\n> Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwisc.hclust.clusters <- cutree(wisc.hclust, k = 2)\ntable(wisc.hclust.clusters, diagnosis)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                    diagnosis\nwisc.hclust.clusters   B   M\n                   1 357 210\n                   2   0   2\n```\n\n\n:::\n:::\n\n\n2 clusters gives better separation between benign and malignant. \n\n## Clustering in PC space\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhc <- hclust(dist(wisc.pr$x[,1:2]), method=\"ward.D2\")\n\nplot(hc)\nabline(h=70, col=\"red\")\n```\n\n::: {.cell-output-display}\n![](class08-mini-lab_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n:::\n\n\n\nCluster membership vector\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrps <- cutree(hc, h=70)\ntable(grps)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ngrps\n  1   2 \n195 374 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(diagnosis)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ndiagnosis\n  B   M \n357 212 \n```\n\n\n:::\n:::\n\n\n\nCross-table to see how my clustering groups correspond to the expert diagnosis vector of M and B values\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(grps, diagnosis)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n    diagnosis\ngrps   B   M\n   1  18 177\n   2 339  35\n```\n\n\n:::\n:::\n\n\n\nPositive => cancer M\nNegative => non-cancer B\n\nTrue = cluster/grp 1\nFalse = grp 2\n\n\nTrue Positive 177 \nFalse Positive 18\nTrue Negative 339\nFalse Negative 35\n\nWe can use our PCA results (wisc.pr) to make predictions on new unseen data. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nurl <- \"https://tinyurl.com/new-samples-CSV\"\nnew <- read.csv(url)\nnpc <- predict(wisc.pr, newdata=new)\nnpc\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n           PC1       PC2        PC3        PC4       PC5        PC6        PC7\n[1,]  2.576616 -3.135913  1.3990492 -0.7631950  2.781648 -0.8150185 -0.3959098\n[2,] -4.754928 -3.009033 -0.1660946 -0.6052952 -1.140698 -1.2189945  0.8193031\n            PC8       PC9       PC10      PC11      PC12      PC13     PC14\n[1,] -0.2307350 0.1029569 -0.9272861 0.3411457  0.375921 0.1610764 1.187882\n[2,] -0.3307423 0.5281896 -0.4855301 0.7173233 -1.185917 0.5893856 0.303029\n          PC15       PC16        PC17        PC18        PC19       PC20\n[1,] 0.3216974 -0.1743616 -0.07875393 -0.11207028 -0.08802955 -0.2495216\n[2,] 0.1299153  0.1448061 -0.40509706  0.06565549  0.25591230 -0.4289500\n           PC21       PC22       PC23       PC24        PC25         PC26\n[1,]  0.1228233 0.09358453 0.08347651  0.1223396  0.02124121  0.078884581\n[2,] -0.1224776 0.01732146 0.06316631 -0.2338618 -0.20755948 -0.009833238\n             PC27        PC28         PC29         PC30\n[1,]  0.220199544 -0.02946023 -0.015620933  0.005269029\n[2,] -0.001134152  0.09638361  0.002795349 -0.019015820\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(wisc.pr$x[,1:2], col=grps)\npoints(npc[,1], npc[,2], col=\"blue\", pch=16, cex=3)\ntext(npc[,1], npc[,2], c(1,2), col=\"white\")\n```\n\n::: {.cell-output-display}\n![](class08-mini-lab_files/figure-html/unnamed-chunk-39-1.png){width=672}\n:::\n:::\n\n\n\n> Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwisc.hclust_ward <- hclust(data.dist, method = \"ward.D2\")\nclusters_ward <- cutree(wisc.hclust_ward, k = 2)\ntable(clusters_ward, diagnosis)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n             diagnosis\nclusters_ward   B   M\n            1  20 164\n            2 337  48\n```\n\n\n:::\n:::\n\n\nI prefer the Ward's method (Ward.D2) because it minimizes within-cluster variance, leading to well-separated, compact clusters. The clustering results show that this method provides the best separation between malignant and benign cases, making it the most effective for this dataset.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method=\"ward.D2\")\n```\n:::\n\n\n\n> Q14. How well does k-means separate the two diagnoses? How does it compare to your hclust results?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwisc.km <- kmeans(data.scaled, centers = 2, nstart = 20)\ntable(wisc.km$cluster, diagnosis)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   diagnosis\n      B   M\n  1  14 175\n  2 343  37\n```\n\n\n:::\n:::\n\n\nThe k-means clustering with k = 2 performs well in separating the two diagnoses.Compared to hierarchical clustering (Ward’s method), k-means provides a similar level of accuracy, but Ward’s method appears more stable for this dataset.\n\n\n> Q15. How well does the newly created model with four clusters separate out the two diagnoses?\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nwisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k = 2)\ntable(wisc.pr.hclust.clusters, diagnosis)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                       diagnosis\nwisc.pr.hclust.clusters   B   M\n                      1  28 188\n                      2 329  24\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nwisc.pr.hclust.clusters_4 <- cutree(wisc.pr.hclust, k = 4)\ntable(wisc.pr.hclust.clusters_4, diagnosis)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                         diagnosis\nwisc.pr.hclust.clusters_4   B   M\n                        1   0  45\n                        2   2  77\n                        3  26  66\n                        4 329  24\n```\n\n\n:::\n:::\n\n\nThe 2-cluster model provides a clearer separation between benign and malignant cases, making it the better choice for distinguishing between the two groups. The 4-cluster modelresults in mixed clusters, where some contain both benign and malignant cases\n\n> Q16. How well do the k-means and hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the table() function to compare the output of each model (wisc.km$cluster and wisc.hclust.clusters) with the vector containing the actual diagnoses\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(wisc.km$cluster, diagnosis)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   diagnosis\n      B   M\n  1  14 175\n  2 343  37\n```\n\n\n:::\n\n```{.r .cell-code}\ntable(wisc.hclust.clusters, diagnosis)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                    diagnosis\nwisc.hclust.clusters   B   M\n                   1 357 210\n                   2   0   2\n```\n\n\n:::\n:::\n\n\nThe K-means clustering model and hierarchical clustering model (Ward's method) both perform well in separating benign and malignant cases.\n\n> Q18. Which of these new patients should we prioritize for follow up based on your results?\n\n\nPatient 1\n\n",
    "supporting": [
      "class08-mini-lab_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}